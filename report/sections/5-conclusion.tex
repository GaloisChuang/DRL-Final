In this project, we started with the classic 2D Tetris using tetrominoes (four-block pieces) and successfully trained an agent to complete the environment using Deep Q-Networks (DQN). We then increased the difficulty by introducing pentominoes (five-block pieces). Although we adjusted various training parameters to help the agent adapt, the overall performance was not as strongâ€”likely due to certain pentomino shapes being inherently more difficult to work with.

Building upon the 2D foundation, we extended the environment into 3D, creating a three-dimensional Tetris game. We selected four types of 3D tetrominoes for training with DQN. The results were impressive: the trained agent achieved average scores exceeding 1,000 points, significantly outperforming random strategies. This also marks one of the first successful applications of DQN to a 3D Tetris environment, demonstrating strong generalization and potential for future development.